#!/usr/bin/env bash
set -euo pipefail

# Task aggregation for pai-lite

script_dir="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
# shellcheck source=lib/common.sh
source "$script_dir/common.sh"
# shellcheck source=lib/triggers.sh
source "$script_dir/triggers.sh"

pai_lite_projects_list() {
  local config
  config="$(pai_lite_config_path)"
  [[ -f "$config" ]] || pai_lite_die "config not found: $config"
  awk '
    function trim(s) {
      sub(/^[[:space:]]+/, "", s)
      sub(/[[:space:]]+$/, "", s)
      return s
    }
    /^[[:space:]]*projects:/ { in_projects=1; next }
    in_projects && $0 ~ /^[[:space:]]*$/ { next }
    in_projects && $0 !~ /^[[:space:]]/ { in_projects=0 }
    in_projects {
      if ($0 ~ /^[[:space:]]*-[[:space:]]*name:/) {
        if (name != "") {
          printf "%s|%s|%s\n", name, repo, issues
        }
        name=$0
        sub(/^[^:]+:[[:space:]]*/, "", name)
        name=trim(name)
        repo=""
        issues="false"
      } else if ($0 ~ /^[[:space:]]*repo:/) {
        repo=$0
        sub(/^[^:]+:[[:space:]]*/, "", repo)
        repo=trim(repo)
      } else if ($0 ~ /^[[:space:]]*issues:/) {
        issues=$0
        sub(/^[^:]+:[[:space:]]*/, "", issues)
        issues=trim(issues)
      }
    }
    END {
      if (name != "") {
        printf "%s|%s|%s\n", name, repo, issues
      }
    }
  ' "$config"
}

yaml_escape() {
  local value="$1"
  value=${value//\\/\\\\}
  value=${value//\"/\\\"}
  printf "%s" "$value"
}

# Sanitize a file path for use in task IDs
# Strips $HOME/ prefix, replaces /, spaces, dots with -
sanitize_path_for_id() {
  local path="$1"
  path="${path/#$HOME\//}"
  path="${path/#\~\//}"
  echo "$path" | tr '/. ' '---' | tr -cd 'a-zA-Z0-9_-'
}

# Generate an 8-char hex fingerprint from task text
# Normalizes: lowercase, strip non-alphanumeric (keep spaces), collapse whitespace, trim
content_fingerprint() {
  local text="$1"
  text="${text,,}"
  text="$(printf '%s' "$text" | tr -cd 'a-z0-9 ' | tr -s ' ' | sed 's/^ //;s/ $//')"
  if command -v md5sum >/dev/null 2>&1; then
    printf '%s' "$text" | md5sum | cut -c1-8
  elif command -v md5 >/dev/null 2>&1; then
    printf '%s' "$text" | md5 -q | cut -c1-8
  else
    printf '%s' "$text" | cksum | awk '{printf "%08x", $1}'
  fi
}

tasks_file_path() {
  pai_lite_ensure_state_repo
  pai_lite_ensure_state_dir
  echo "$(pai_lite_state_harness_dir)/tasks.yaml"
}

tasks_sync() {
  local file tmp timestamp
  file="$(tasks_file_path)"
  tmp="${file}.tmp"
  timestamp="$(date -u +"%Y-%m-%dT%H:%MZ")"

  pai_lite_require_cmd gh

  {
    echo "# Generated by pai-lite"
    echo "generated_at: \"$timestamp\""
    echo "tasks:"

    # GitHub issues from projects config
    while IFS='|' read -r name repo issues; do
      local repo_name
      repo_name="${repo##*/}"

      if [[ "$issues" == "true" ]]; then
        gh issue list -R "$repo" --state open --limit 200 \
          --json number,title,url,labels \
          --jq '.[] | [.number, .title, (.labels|map(.name)|join(",")), .url] | @tsv' \
          | while IFS=$'\t' read -r number title labels url; do
              local id escaped_title
              id="gh-${repo_name}-${number}"
              escaped_title="$(yaml_escape "$title")"
              echo "  - id: $id"
              echo "    title: \"$escaped_title\""
              echo "    source: github"
              echo "    repo: $repo"
              echo "    url: $url"
              echo "    state: open"
              if [[ -n "$labels" ]]; then
                echo "    labels: \"$(yaml_escape "$labels")\""
              else
                echo "    labels: \"\""
              fi
            done
      fi
    done < <(pai_lite_projects_list)

    # Scan watch paths for checkboxes and TODOs
    # Only paths from rules with action "tasks sync" are scanned
    while IFS='|' read -r rule_action rule_paths_csv; do
      [[ "$rule_action" == "tasks sync" ]] || continue
      IFS=',' read -ra watch_paths <<< "$rule_paths_csv"
      for watch_path in "${watch_paths[@]}"; do
        [[ -n "$watch_path" ]] || continue
        local expanded_path="${watch_path/#\~/$HOME}"
        if [[ ! -f "$expanded_path" ]]; then
          pai_lite_warn "watch path not found: $watch_path"
          continue
        fi
        local path_id
        path_id="$(sanitize_path_for_id "$expanded_path")"
        local line_no=0
        # shellcheck disable=SC2094 # expanded_path is read as input, echoed as a string — not written
        while IFS= read -r line || [[ -n "$line" ]]; do
          line_no=$((line_no + 1))
          if [[ "$line" =~ ^[[:space:]]*[-*][[:space:]]*\[[[:space:]]\][[:space:]]*(.+)$ ]]; then
            local task_text id escaped fingerprint
            task_text="${BASH_REMATCH[1]:-}"
            fingerprint="$(content_fingerprint "$task_text")"
            id="watch-${path_id}-${fingerprint}"
            escaped="$(yaml_escape "$task_text")"
            echo "  - id: $id"
            echo "    title: \"$escaped\""
            echo "    source: watch"
            echo "    path: \"$(yaml_escape "$expanded_path")\""
            echo "    line: $line_no"
          elif [[ "$line" =~ TODO[:[:space:]]*(.+)$ ]]; then
            local task_text id escaped fingerprint
            task_text="${BASH_REMATCH[1]:-}"
            fingerprint="$(content_fingerprint "$task_text")"
            id="watch-${path_id}-${fingerprint}"
            escaped="$(yaml_escape "$task_text")"
            echo "  - id: $id"
            echo "    title: \"$escaped\""
            echo "    source: watch"
            echo "    path: \"$(yaml_escape "$expanded_path")\""
            echo "    line: $line_no"
          fi
        done < "$expanded_path"
      done
    done < <(trigger_get_watch_rules)
  } > "$tmp"

  mv "$tmp" "$file"
  echo "Wrote tasks to $file"

  # Automatically convert to individual task files for the flow engine
  tasks_convert

  # Queue elaboration for any new ready tasks (deterministic, deduplicates)
  tasks_queue_elaborations
}

tasks_list() {
  local file
  file="$(tasks_file_path)"
  [[ -f "$file" ]] || pai_lite_die "tasks file not found: $file (run: pai-lite tasks sync)"
  awk '
    /^[[:space:]]*-[[:space:]]*id:/ {
      id=$0
      sub(/.*id:[[:space:]]*/, "", id)
      gsub(/\"/, "", id)
      next
    }
    /^[[:space:]]*title:/ {
      if (id != "") {
        title=$0
        sub(/.*title:[[:space:]]*/, "", title)
        gsub(/^\"|\"$/, "", title)
        print id " - " title
        id=""
      }
    }
  ' "$file"
}

tasks_show() {
  local task_id="$1"
  local file
  file="$(tasks_file_path)"
  [[ -f "$file" ]] || pai_lite_die "tasks file not found: $file (run: pai-lite tasks sync)"
  awk -v target="$task_id" '
    /^[[:space:]]*-[[:space:]]*id:/ {
      current=$0
      sub(/.*id:[[:space:]]*/, "", current)
      gsub(/\"/, "", current)
      if (in_block && current != target) { exit }
      in_block=(current==target)
    }
    in_block { print }
  ' "$file"
}

#------------------------------------------------------------------------------
# Task file directory (individual task-*.md files for flow engine)
#------------------------------------------------------------------------------

tasks_dir_path() {
  pai_lite_ensure_state_repo
  pai_lite_ensure_state_dir
  echo "$(pai_lite_state_harness_dir)/tasks"
}

#------------------------------------------------------------------------------
# Convert tasks.yaml to individual task files
# Creates task-*.md files with YAML frontmatter for the flow engine
#------------------------------------------------------------------------------

tasks_convert() {
  local yaml_file tasks_dir
  yaml_file="$(tasks_file_path)"
  tasks_dir="$(tasks_dir_path)"

  [[ -f "$yaml_file" ]] || pai_lite_die "tasks.yaml not found (run: pai-lite tasks sync first)"

  mkdir -p "$tasks_dir"

  local today
  today=$(date +%Y-%m-%d)

  local count=0
  local current_id="" current_title="" current_source="" current_repo=""
  local current_url="" current_labels="" current_path=""

  # Parse tasks.yaml and create individual files
  while IFS= read -r line; do
    # New task entry
    if [[ "$line" =~ ^[[:space:]]*-[[:space:]]*id:[[:space:]]*(.+)$ ]]; then
      # Capture new ID immediately (BASH_REMATCH is global and may be modified by tasks_write_file)
      local new_id="${BASH_REMATCH[1]:-}"
      # Write previous task if exists
      if [[ -n "$current_id" ]]; then
        tasks_write_file "$current_id" "$current_title" "$current_source" \
          "$current_repo" "$current_url" "$current_labels" "$today" "$tasks_dir" "$current_path"
        count=$((count + 1))
      fi
      current_id="$new_id"
      current_id="${current_id//\"/}"
      current_title=""
      current_source=""
      current_repo=""
      current_url=""
      current_labels=""
      current_path=""
    elif [[ "$line" =~ ^[[:space:]]*title:[[:space:]]*\"?(.+)\"?$ ]]; then
      current_title="${BASH_REMATCH[1]:-}"
      current_title="${current_title%\"}"
    elif [[ "$line" =~ ^[[:space:]]*source:[[:space:]]*(.+)$ ]]; then
      current_source="${BASH_REMATCH[1]:-}"
    elif [[ "$line" =~ ^[[:space:]]*repo:[[:space:]]*(.+)$ ]]; then
      current_repo="${BASH_REMATCH[1]:-}"
    elif [[ "$line" =~ ^[[:space:]]*url:[[:space:]]*(.+)$ ]]; then
      current_url="${BASH_REMATCH[1]:-}"
    elif [[ "$line" =~ ^[[:space:]]*labels:[[:space:]]*\"?(.*)\"?$ ]]; then
      current_labels="${BASH_REMATCH[1]:-}"
      current_labels="${current_labels%\"}"
    elif [[ "$line" =~ ^[[:space:]]*path:[[:space:]]*\"?(.+)\"?$ ]]; then
      current_path="${BASH_REMATCH[1]:-}"
      current_path="${current_path%\"}"
    fi
  done < "$yaml_file"

  # Write last task
  if [[ -n "$current_id" ]]; then
    tasks_write_file "$current_id" "$current_title" "$current_source" \
      "$current_repo" "$current_url" "$current_labels" "$today" "$tasks_dir" "$current_path"
    count=$((count + 1))
  fi

  echo "Created $count task files in $tasks_dir"
}

# Helper: Write individual task file
tasks_write_file() {
  local id="$1" title="$2" source="$3" repo="$4" url="$5" labels="$6" today="$7" dir="$8"
  local watch_path="${9:-}"

  # Skip if file already exists (don't overwrite user edits)
  local file="$dir/${id}.md"
  if [[ -f "$file" ]]; then
    pai_lite_info "skipping existing: $id"
    return
  fi

  # Migration: if this is a fingerprint-based watch task, look for old line-number-based file
  if [[ "$source" == "watch" && "$id" =~ ^watch-(.+)-[0-9a-f]{8}$ ]]; then
    local path_prefix="${BASH_REMATCH[1]}"
    local old_file=""
    for candidate in "$dir"/watch-"${path_prefix}"-*.md; do
      [[ -f "$candidate" ]] || continue
      local candidate_basename
      candidate_basename="$(basename "$candidate" .md)"
      # Check if candidate has a numeric suffix (old line-number format)
      local candidate_suffix="${candidate_basename##watch-${path_prefix}-}"
      if [[ "$candidate_suffix" =~ ^[0-9]+$ ]]; then
        # Verify title matches
        local candidate_title
        candidate_title="$(awk '/^title:/ { sub(/^title:[[:space:]]*"?/, ""); sub(/"?$/, ""); print; exit }' "$candidate")"
        if [[ "$candidate_title" == "$title" ]]; then
          old_file="$candidate"
          break
        fi
      fi
    done

    if [[ -n "$old_file" ]]; then
      mv "$old_file" "$file"
      # Update the id: field in frontmatter
      if [[ "$(uname -s)" == "Darwin" ]]; then
        sed -i '' "s/^id: .*/id: $id/" "$file"
      else
        sed -i "s/^id: .*/id: $id/" "$file"
      fi
      pai_lite_info "migrated: $(basename "$old_file" .md) -> $id"
      return
    fi
  fi

  # Infer priority from labels
  local priority="B"
  if [[ "$labels" =~ (urgent|critical|high|priority) ]]; then
    priority="A"
  elif [[ "$labels" =~ (low|minor|nice-to-have) ]]; then
    priority="C"
  fi

  # Infer project from repo
  local project=""
  if [[ -n "$repo" ]]; then
    project="${repo##*/}"
  fi

  # Write the task file
  cat > "$file" << EOF
---
id: $id
title: "$title"
project: $project
status: ready
priority: $priority
deadline: null
dependencies:
  blocks: []
  blocked_by: []
effort: medium
context: $project
slot: null
adapter: null
created: $today
started: null
completed: null
source: $source
EOF

  if [[ -n "$url" ]]; then
    echo "url: $url" >> "$file"
  fi
  if [[ "$source" == "github" && "$id" =~ gh-[^-]+-([0-9]+)$ ]]; then
    echo "github_issue: ${BASH_REMATCH[1]:-}" >> "$file"
  fi

  cat >> "$file" << EOF
---

# $title

## Context

$(if [[ -n "$url" ]]; then echo "Source: $url"; fi)
$(if [[ -n "$labels" ]]; then echo "Labels: $labels"; fi)

## Acceptance Criteria

- [ ] TBD

## Notes

EOF

  pai_lite_info "created: $id"
}

#------------------------------------------------------------------------------
# Create a new task manually
#------------------------------------------------------------------------------

tasks_create() {
  local title="$1"
  local project="${2:-personal}"
  local priority="${3:-B}"

  local tasks_dir
  tasks_dir="$(tasks_dir_path)"
  mkdir -p "$tasks_dir"

  # Generate task ID
  local count today id
  count=$(find "$tasks_dir" -name "*.md" 2>/dev/null | wc -l | tr -d ' ')
  count=$((count + 1))
  today=$(date +%Y-%m-%d)
  id="task-$(printf '%03d' "$count")"

  local file="$tasks_dir/${id}.md"

  cat > "$file" << EOF
---
id: $id
title: "$title"
project: $project
status: ready
priority: $priority
deadline: null
dependencies:
  blocks: []
  blocked_by: []
effort: medium
context: $project
slot: null
adapter: null
created: $today
started: null
completed: null
---

# $title

## Context

Created manually via pai-lite.

## Acceptance Criteria

- [ ] TBD

## Notes

EOF

  echo "Created task: $file"
  echo "ID: $id"
}

#------------------------------------------------------------------------------
# List task files (not tasks.yaml entries)
#------------------------------------------------------------------------------

tasks_files_list() {
  local tasks_dir
  tasks_dir="$(tasks_dir_path)"

  if [[ ! -d "$tasks_dir" ]]; then
    echo "No task files yet (run: pai-lite tasks convert)"
    return
  fi

  for file in "$tasks_dir"/*.md; do
    [[ -f "$file" ]] || continue
    # Extract id and title from frontmatter
    local id title status priority
    id=$(awk '/^id:/ { print $2; exit }' "$file")
    title=$(awk '/^title:/ { sub(/^title:[[:space:]]*"?/, ""); sub(/"?$/, ""); print; exit }' "$file")
    status=$(awk '/^status:/ { print $2; exit }' "$file")
    priority=$(awk '/^priority:/ { print $2; exit }' "$file")
    echo "$id ($priority) [$status] $title"
  done
}

#------------------------------------------------------------------------------
# Create sample task files for testing
#------------------------------------------------------------------------------

tasks_create_samples() {
  local tasks_dir
  tasks_dir="$(tasks_dir_path)"
  mkdir -p "$tasks_dir"

  local today
  today=$(date +%Y-%m-%d)

  # Sample 1: Ready high-priority task
  cat > "$tasks_dir/task-001.md" << EOF
---
id: task-001
title: "Implement user authentication"
project: sample-app
status: ready
priority: A
deadline: null
dependencies:
  blocks: [task-002, task-003]
  blocked_by: []
effort: large
context: auth
slot: null
adapter: null
created: $today
started: null
completed: null
---

# Implement user authentication

## Context

Sample task for testing pai-lite flow engine.

## Acceptance Criteria

- [ ] Login form works
- [ ] Session management implemented
- [ ] Logout functionality

## Notes

This is a sample task.
EOF

  # Sample 2: Blocked task
  cat > "$tasks_dir/task-002.md" << EOF
---
id: task-002
title: "Add password reset flow"
project: sample-app
status: ready
priority: B
deadline: null
dependencies:
  blocks: []
  blocked_by: [task-001]
effort: medium
context: auth
slot: null
adapter: null
created: $today
started: null
completed: null
---

# Add password reset flow

## Context

Depends on authentication being implemented first.

## Acceptance Criteria

- [ ] Reset email sent
- [ ] Token validation works
- [ ] Password update successful

## Notes

Blocked by task-001.
EOF

  # Sample 3: Task with deadline
  cat > "$tasks_dir/task-003.md" << EOF
---
id: task-003
title: "Write API documentation"
project: sample-app
status: ready
priority: B
deadline: $(date -v+14d +%Y-%m-%d 2>/dev/null || date -d "+14 days" +%Y-%m-%d 2>/dev/null || echo "2026-02-15")
dependencies:
  blocks: []
  blocked_by: [task-001]
effort: medium
context: docs
slot: null
adapter: null
created: $today
started: null
completed: null
---

# Write API documentation

## Context

Document the auth API endpoints.

## Acceptance Criteria

- [ ] All endpoints documented
- [ ] Examples provided
- [ ] Published to docs site

## Notes

Has a deadline for review.
EOF

  # Sample 4: In-progress stalled task
  local started_date
  started_date=$(date -v-10d +%Y-%m-%d 2>/dev/null || date -d "-10 days" +%Y-%m-%d 2>/dev/null || echo "2026-01-22")

  cat > "$tasks_dir/task-004.md" << EOF
---
id: task-004
title: "Refactor database layer"
project: sample-app
status: in-progress
priority: B
deadline: null
dependencies:
  blocks: []
  blocked_by: []
effort: large
context: backend
slot: null
adapter: null
created: $today
started: $started_date
completed: null
---

# Refactor database layer

## Context

Been working on this for a while...

## Acceptance Criteria

- [ ] New ORM integrated
- [ ] Migrations working
- [ ] Tests passing

## Notes

This task is intentionally stalled for testing critical view.
EOF

  # Sample 5: Low priority task
  cat > "$tasks_dir/task-005.md" << EOF
---
id: task-005
title: "Add dark mode support"
project: sample-app
status: ready
priority: C
deadline: null
dependencies:
  blocks: []
  blocked_by: []
effort: small
context: ui
slot: null
adapter: null
created: $today
started: null
completed: null
---

# Add dark mode support

## Context

Nice-to-have feature.

## Acceptance Criteria

- [ ] Theme toggle works
- [ ] Colors look good

## Notes

Low priority.
EOF

  echo "Created 5 sample task files in $tasks_dir"
  echo ""
  echo "Test with:"
  echo "  pai-lite flow ready      # Should show task-001, task-005"
  echo "  pai-lite flow blocked    # Should show task-002, task-003"
  echo "  pai-lite flow critical   # Should show deadline + stalled task"
  echo "  pai-lite flow impact task-001  # Should show task-002, task-003"
}

#------------------------------------------------------------------------------
# Detect tasks that need elaboration
# A task needs elaboration if:
#   - It has "- [ ] TBD" in Acceptance Criteria (template marker)
#   - OR it lacks an "elaborated:" field in frontmatter
# This has no false negatives on tasks created by tasks_convert() or tasks_create()
#------------------------------------------------------------------------------

tasks_needs_elaboration() {
  local tasks_dir
  tasks_dir="$(tasks_dir_path)"

  if [[ ! -d "$tasks_dir" ]]; then
    return
  fi

  for file in "$tasks_dir"/*.md; do
    [[ -f "$file" ]] || continue

    local id needs_elab=false

    # Extract task ID
    id=$(awk '/^id:/ { print $2; exit }' "$file")

    # Check 1: Has "elaborated:" in frontmatter?
    if ! grep -q '^elaborated:' "$file"; then
      needs_elab=true
    fi

    # Check 2: Has "- [ ] TBD" marker? (template placeholder)
    if grep -q '^\- \[ \] TBD$' "$file"; then
      needs_elab=true
    fi

    if [[ "$needs_elab" == "true" ]]; then
      echo "$id"
    fi
  done
}

#------------------------------------------------------------------------------
# Queue elaboration requests for unprocessed ready tasks
# Deterministic: checks queue to avoid duplicates, then queues each task
#------------------------------------------------------------------------------

tasks_queue_elaborations() {
  local tasks_dir
  tasks_dir="$(tasks_dir_path)"
  [[ -d "$tasks_dir" ]] || return 0

  # Get list of tasks needing elaboration
  local needs_elab
  needs_elab="$(tasks_needs_elaboration)"
  [[ -n "$needs_elab" ]] || return 0

  # Read current queue to avoid duplicate requests
  local queue_file already_queued=""
  queue_file="$(pai_lite_queue_file)"
  if [[ -f "$queue_file" ]] && [[ -s "$queue_file" ]]; then
    already_queued="$(grep '"action":"elaborate"' "$queue_file" 2>/dev/null || true)"
  fi

  local count=0
  while IFS= read -r task_id; do
    [[ -n "$task_id" ]] || continue

    # Only queue ready tasks (not blocked, not in-progress, not done)
    local file="$tasks_dir/${task_id}.md"
    [[ -f "$file" ]] || continue
    local status
    status=$(awk '/^status:/ { print $2; exit }' "$file")
    if [[ "$status" != "ready" ]]; then
      continue
    fi

    # Skip if already in queue
    if [[ -n "$already_queued" ]] && echo "$already_queued" | grep -q "\"task\":\"$task_id\""; then
      continue
    fi

    pai_lite_queue_request "elaborate" "\"task\":\"$task_id\"" >/dev/null
    count=$((count + 1))
  done <<< "$needs_elab"

  if [[ $count -gt 0 ]]; then
    pai_lite_info "Queued $count task(s) for elaboration"
  fi
}

# Check if a specific task needs elaboration
tasks_check_elaboration() {
  local task_id="$1"
  local tasks_dir file
  tasks_dir="$(tasks_dir_path)"
  file="$tasks_dir/${task_id}.md"

  if [[ ! -f "$file" ]]; then
    pai_lite_die "task file not found: $file"
  fi

  # Check for elaborated field
  if ! grep -q '^elaborated:' "$file"; then
    echo "needs-elaboration"
    return 0
  fi

  # Check for TBD marker
  if grep -q '^\- \[ \] TBD$' "$file"; then
    echo "needs-elaboration"
    return 0
  fi

  echo "elaborated"
  return 0
}

#------------------------------------------------------------------------------
# Migrate cross-references after fingerprint ID migration
# Handles after-the-fact cleanup when both old (numeric suffix) and new
# (hex suffix) files already exist. Also cleans up orphan template files.
#------------------------------------------------------------------------------

tasks_migrate_refs() {
  local harness_dir
  harness_dir="$(pai_lite_state_harness_dir)"
  local tasks_dir="$harness_dir/tasks"

  [[ -d "$tasks_dir" ]] || pai_lite_die "tasks dir not found: $tasks_dir"

  local is_darwin=false
  if [[ "$(uname -s)" == "Darwin" ]]; then
    is_darwin=true
  fi

  # Phase 1: Rename old (numeric suffix) files to fingerprint-based IDs
  # Computes fingerprint from the old file's title. If a new template file
  # with that fingerprint exists, it's removed (old file has user edits).
  local mappings=()
  for old_file in "$tasks_dir"/watch-*-*.md; do
    [[ -f "$old_file" ]] || continue
    local old_id
    old_id="$(basename "$old_file" .md)"

    local suffix="${old_id##*-}"
    [[ "$suffix" =~ ^[0-9]+$ ]] || continue

    local old_title
    old_title="$(awk '/^title:/ { sub(/^title:[[:space:]]*"?/, ""); sub(/"?$/, ""); print; exit }' "$old_file")"
    [[ -n "$old_title" ]] || continue

    local fp new_id
    fp="$(content_fingerprint "$old_title")"
    local path_prefix="${old_id%-*}"
    new_id="${path_prefix}-${fp}"

    local new_file="$tasks_dir/${new_id}.md"

    if [[ "$old_id" == "$new_id" ]]; then
      continue
    fi

    # Remove template file if it exists (old file is authoritative)
    if [[ -f "$new_file" ]]; then
      rm "$new_file"
      pai_lite_info "removed template: $new_id"
    fi

    mv "$old_file" "$new_file"
    if $is_darwin; then
      sed -i '' "s/^id: .*/id: $new_id/" "$new_file"
    else
      sed -i "s/^id: .*/id: $new_id/" "$new_file"
    fi
    pai_lite_info "migrated: $old_id -> $new_id"
    mappings+=("$old_id|$new_id")
  done

  # Phase 2: Clean up orphan template files (hex-suffix files created today
  # with TBD acceptance criteria that don't correspond to any old file)
  local orphans_removed=0
  for f in "$tasks_dir"/watch-*-*.md; do
    [[ -f "$f" ]] || continue
    local fid
    fid="$(basename "$f" .md)"
    local fsuffix="${fid##*-}"
    [[ "$fsuffix" =~ ^[0-9a-f]{8}$ ]] || continue
    # Only remove if it looks like an unedited template
    if grep -q '^\- \[ \] TBD$' "$f" 2>/dev/null; then
      rm "$f"
      pai_lite_info "removed orphan template: $fid"
      orphans_removed=$((orphans_removed + 1))
    fi
  done

  if [[ ${#mappings[@]} -eq 0 && $orphans_removed -eq 0 ]]; then
    echo "No migrations needed"
    return
  fi

  if [[ $orphans_removed -gt 0 ]]; then
    echo "Removed $orphans_removed orphan template file(s)"
  fi

  if [[ ${#mappings[@]} -eq 0 ]]; then
    echo "No ID migrations to apply"
    return
  fi

  echo "Migrated ${#mappings[@]} file(s), updating cross-references..."

  # Phase 3: Replace old IDs in all task files + harness-level md/yaml
  local files_to_update=()
  for f in "$tasks_dir"/*.md; do
    [[ -f "$f" ]] && files_to_update+=("$f")
  done
  for f in "$harness_dir"/*.md "$harness_dir"/*.yaml; do
    [[ -f "$f" ]] && files_to_update+=("$f")
  done

  local total_replacements=0
  for mapping in "${mappings[@]}"; do
    local old_id="${mapping%%|*}"
    local new_id="${mapping##*|}"
    for f in "${files_to_update[@]}"; do
      if grep -q "$old_id" "$f" 2>/dev/null; then
        if $is_darwin; then
          sed -i '' "s|$old_id|$new_id|g" "$f"
        else
          sed -i "s|$old_id|$new_id|g" "$f"
        fi
        total_replacements=$((total_replacements + 1))
      fi
    done
  done

  echo "Updated $total_replacements file(s) with new IDs"

  echo ""
  echo "Migration mapping:"
  for mapping in "${mappings[@]}"; do
    local old_id="${mapping%%|*}"
    local new_id="${mapping##*|}"
    echo "  $old_id -> $new_id"
  done
}

#------------------------------------------------------------------------------
# Merge tasks: mark source tasks as merged into a target task
#------------------------------------------------------------------------------

tasks_merge() {
  local target_id="$1"
  shift
  local source_ids=("$@")

  [[ -n "$target_id" ]] || pai_lite_die "target task ID required"
  [[ ${#source_ids[@]} -gt 0 ]] || pai_lite_die "at least one source task ID required"

  local tasks_dir
  tasks_dir="$(tasks_dir_path)"

  # Validate all IDs exist
  local target_file="$tasks_dir/${target_id}.md"
  if [[ ! -f "$target_file" ]]; then
    pai_lite_die "target task not found: $target_id"
  fi

  for src_id in "${source_ids[@]}"; do
    local src_file="$tasks_dir/${src_id}.md"
    if [[ ! -f "$src_file" ]]; then
      pai_lite_die "source task not found: $src_id"
    fi
    if [[ "$src_id" == "$target_id" ]]; then
      pai_lite_die "cannot merge a task into itself: $src_id"
    fi
  done

  local merged_count=0

  for src_id in "${source_ids[@]}"; do
    local src_file="$tasks_dir/${src_id}.md"

    # Update source: set status to merged, add merged_into field
    task_update_frontmatter "$src_id" "status" "merged"
    task_add_frontmatter "$src_id" "merged_into" "$target_id"
    task_update_frontmatter "$src_id" "slot" "null"

    # Transfer dependencies from source to target
    tasks_merge_transfer_deps "$src_id" "$target_id"

    # Update any tasks that reference source in blocked_by → replace with target
    tasks_merge_repoint_deps "$src_id" "$target_id"

    merged_count=$((merged_count + 1))
    echo "Merged: $src_id -> $target_id"
  done

  # Add merged_from list to target
  local merged_list
  merged_list="[$(printf '%s' "${source_ids[*]}" | tr ' ' ',')]"
  # If target already has merged_from, we need to append
  if grep -q '^merged_from:' "$target_file"; then
    # Read existing list, append new IDs
    local existing
    existing="$(awk '/^merged_from:/ { sub(/^merged_from:[[:space:]]*/, ""); print; exit }' "$target_file")"
    # Simple approach: combine the bracket contents
    existing="${existing#\[}"
    existing="${existing%\]}"
    local new_items
    new_items="$(printf '%s' "${source_ids[*]}" | tr ' ' ',')"
    if [[ -n "$existing" ]]; then
      merged_list="[$existing, $new_items]"
    else
      merged_list="[$new_items]"
    fi
    task_update_frontmatter "$target_id" "merged_from" "$merged_list"
  else
    task_add_frontmatter "$target_id" "merged_from" "$merged_list"
  fi

  echo ""
  echo "Merged $merged_count task(s) into $target_id"
}

# Transfer blocks/blocked_by from source to target (internal helper)
tasks_merge_transfer_deps() {
  local src_id="$1" target_id="$2"
  local tasks_dir
  tasks_dir="$(tasks_dir_path)"
  local src_file="$tasks_dir/${src_id}.md"
  local target_file="$tasks_dir/${target_id}.md"

  pai_lite_require_cmd yq

  # Extract source dependencies
  local src_blocks src_blocked_by
  src_blocks="$(awk '/^---$/{ if(++c==2) exit } c==1' "$src_file" | yq -r '.dependencies.blocks // [] | .[]' 2>/dev/null || true)"
  src_blocked_by="$(awk '/^---$/{ if(++c==2) exit } c==1' "$src_file" | yq -r '.dependencies.blocked_by // [] | .[]' 2>/dev/null || true)"

  # Extract target dependencies
  local tgt_blocks tgt_blocked_by
  tgt_blocks="$(awk '/^---$/{ if(++c==2) exit } c==1' "$target_file" | yq -r '.dependencies.blocks // [] | .[]' 2>/dev/null || true)"
  tgt_blocked_by="$(awk '/^---$/{ if(++c==2) exit } c==1' "$target_file" | yq -r '.dependencies.blocked_by // [] | .[]' 2>/dev/null || true)"

  # Merge blocks: add source's blocks to target (skip duplicates and self-refs)
  local new_blocks="$tgt_blocks"
  while IFS= read -r dep; do
    [[ -n "$dep" ]] || continue
    [[ "$dep" != "$target_id" ]] || continue
    [[ "$dep" != "$src_id" ]] || continue
    if ! echo "$new_blocks" | grep -qx "$dep" 2>/dev/null; then
      new_blocks="$(printf '%s\n%s' "$new_blocks" "$dep")"
    fi
  done <<< "$src_blocks"

  # Merge blocked_by: add source's blocked_by to target
  local new_blocked_by="$tgt_blocked_by"
  while IFS= read -r dep; do
    [[ -n "$dep" ]] || continue
    [[ "$dep" != "$target_id" ]] || continue
    [[ "$dep" != "$src_id" ]] || continue
    if ! echo "$new_blocked_by" | grep -qx "$dep" 2>/dev/null; then
      new_blocked_by="$(printf '%s\n%s' "$new_blocked_by" "$dep")"
    fi
  done <<< "$src_blocked_by"

  # Format as YAML arrays and update target
  local blocks_yaml blocked_by_yaml
  blocks_yaml="[$(echo "$new_blocks" | grep -v '^$' | paste -sd',' - 2>/dev/null || echo '')]"
  blocked_by_yaml="[$(echo "$new_blocked_by" | grep -v '^$' | paste -sd',' - 2>/dev/null || echo '')]"

  # Use sed to update the nested dependency fields
  local tmp="${target_file}.tmp"
  awk -v blocks="$blocks_yaml" -v blocked_by="$blocked_by_yaml" '
    /^[[:space:]]*blocks:/ && in_deps { print "  blocks: " blocks; next }
    /^[[:space:]]*blocked_by:/ && in_deps { print "  blocked_by: " blocked_by; next }
    /^dependencies:/ { in_deps=1 }
    /^[a-z]/ && !/^dependencies:/ { in_deps=0 }
    { print }
  ' "$target_file" > "$tmp" && mv "$tmp" "$target_file"
}

# Repoint other tasks' blocked_by references from source to target (internal helper)
tasks_merge_repoint_deps() {
  local src_id="$1" target_id="$2"
  local tasks_dir
  tasks_dir="$(tasks_dir_path)"

  local is_darwin=false
  if [[ "$(uname -s)" == "Darwin" ]]; then
    is_darwin=true
  fi

  for f in "$tasks_dir"/*.md; do
    [[ -f "$f" ]] || continue
    local fid
    fid="$(basename "$f" .md)"
    # Skip source and target themselves
    [[ "$fid" != "$src_id" && "$fid" != "$target_id" ]] || continue
    # Replace source ID with target ID in blocked_by/blocks references
    if grep -q "$src_id" "$f" 2>/dev/null; then
      if $is_darwin; then
        sed -i '' "s|$src_id|$target_id|g" "$f"
      else
        sed -i "s|$src_id|$target_id|g" "$f"
      fi
    fi
  done
}

#------------------------------------------------------------------------------
# Find potential duplicate tasks by title fingerprint
#------------------------------------------------------------------------------

tasks_duplicates() {
  local tasks_dir
  tasks_dir="$(tasks_dir_path)"

  if [[ ! -d "$tasks_dir" ]]; then
    echo "No task files yet"
    return
  fi

  # Collect fingerprint -> task IDs mapping
  # Use temp files since associative arrays in subshells are tricky
  local tmpdir
  tmpdir="$(mktemp -d)"

  for file in "$tasks_dir"/*.md; do
    [[ -f "$file" ]] || continue
    local id status title
    id=$(awk '/^id:/ { print $2; exit }' "$file")
    status=$(awk '/^status:/ { print $2; exit }' "$file")

    # Skip terminal states
    case "$status" in
      done|abandoned|merged) continue ;;
    esac

    title=$(awk '/^title:/ { sub(/^title:[[:space:]]*"?/, ""); sub(/"?$/, ""); print; exit }' "$file")
    [[ -n "$title" ]] || continue

    local fp
    fp="$(content_fingerprint "$title")"

    # Append to fingerprint group file
    echo "$id|$title" >> "$tmpdir/$fp"
  done

  # Find groups with 2+ entries
  local found=false
  local group_num=0
  for fp_file in "$tmpdir"/*; do
    [[ -f "$fp_file" ]] || continue
    local count
    count=$(wc -l < "$fp_file" | tr -d ' ')
    if [[ "$count" -ge 2 ]]; then
      found=true
      group_num=$((group_num + 1))
      echo "Group $group_num (fingerprint: $(basename "$fp_file")):"
      while IFS='|' read -r id title; do
        echo "  $id  \"$title\""
      done < "$fp_file"
      # Suggest merge command using first entry as target
      local first_id
      first_id="$(head -1 "$fp_file" | cut -d'|' -f1)"
      local other_ids
      other_ids="$(tail -n +2 "$fp_file" | cut -d'|' -f1 | tr '\n' ' ')"
      echo "  -> pai-lite tasks merge $first_id $other_ids"
      echo ""
    fi
  done

  rm -rf "$tmpdir"

  if ! $found; then
    echo "No duplicate tasks found"
  fi
}
